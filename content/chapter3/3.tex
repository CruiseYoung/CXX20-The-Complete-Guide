

There are multiple reasons why using requirements as constraints can be useful:

\begin{itemize}
\item
Constraints help us to understand the restrictions on templates and to get more understandable error messages when requirements are broken.

\item
Constraints can be used to disable generic code for cases where the code does not make sense:
\begin{itemize}
\item
For some types, generic code might compile but not do the right thing.

\item
We might have to fix overload resolution, which decides which operation to call if there are multiple valid options.
\end{itemize}

\item
Constraints can be used to overload or specialize generic code so that different code is compiled for different types.
\end{itemize}

Let us take a closer look at these reasons by developing another example step by step. This way, we can also introduce a couple of additional details about constraints, requirements, and concepts.

\mySubsubsection{3.3.1}{Using Concepts to Understand Code and Error Messages}

Assume we want to write generic code that inserts the value of an object into a collection. Thanks to templates, we can implement it once as generic code that is compiled for the types of passed objects once we know them:

\begin{cpp}
template<typename Coll, typename T>
void add(Coll& coll, const T& val)
{
	coll.push_back(val);
}
\end{cpp}

This code does not always compile. There is an implicit requirement for the types of the passed arguments: the call to push\_back() for the value of type T has to be supported for the container of type Coll.

You could also argue that this is a combination of multiple basic requirements:

\begin{itemize}
\item
Type Coll has to support push\_back().

\item
There has to be a conversion from type T to the type of the elements of Coll.

\item
If the passed argument has the element type of Coll, that type has to support copying (because a new element is initialized with the passed value).
\end{itemize}

If any of these requirements are broken, the code does not compile. For example:


\begin{cpp}
std::vector<int> vec;
add(vec, 42); // OK
add(vec, "hello"); // ERROR: no conversion from string literal to int

std::set<int> coll;
add(coll, 42); // ERROR: no push_back() supported by std::set<>

std::vector<std::atomic<int>> aiVec;
std::atomic<int> ai{42};
add(aiVec, ai); // ERROR: cannot copy/move atomics
\end{cpp}


When compilation fails, error messages can be very clear, such as when the member push\_back() is not found on the top level of a template:

{\footnotesize
\begin{shell}
prog.cpp: In instantiation of ’void add(Coll&, const T&)
             [with Coll = std::__debug::set<int>; T = int]’:
prog.cpp:17:18:     required from here
prog.cpp:11:8: error: ’class std::set<int>’ has no member named ’push_back’
  11 | coll.push_back(val);
      | ~~~~~^~~~~~~~~
\end{shell}
}

However, generic error messages can also be very tough to read and understand. For example, when the compiler has to deal with the broken requirement that copying is not supported, the problem is detected in the deep darkness of the implementation of std::vector<>. We get between 40 and 90 lines of error messages where we have to look carefully for the broken requirement:

{\footnotesize
\begin{shell}
...
prog.cpp:11:17: required from ’void add(Coll&, const T&)
                    [with Coll = std::vector<std::atomic<int> >; T = std::atomic<int>]’
prog.cpp:25:18:     required from here
.../include/bits/stl_construct.h:96:17:
    error: use of deleted function
’std::atomic<int>::atomic(const std::atomic<int>&)’
    96 | -> decltype(::new((void*)0) _Tp(std::declval<_Args>()...))
        |                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
...
\end{shell}
}


You might think that you can improve the situation by defining and using a concept that checks whether you can perform the push\_back() call:

\begin{cpp}
template<typename Coll, typename T>
concept SupportsPushBack = requires(Coll c, T v) {
	c.push_back(v);
};

template<typename Coll, typename T>
requires SupportsPushBack<Coll, T>
void add(Coll& coll, const T& val)
{
	coll.push_back(val);
}
\end{cpp}

The error message for not finding push\_back() might now be as follows:

{\scriptsize
\begin{shell}
prog.cpp:27:4: error: no matching function for call to ’add(std::set<int>&, int)’
    27 | add(coll, 42);
        | ~~~^~~~~~~~~~
prog.cpp:14:6: note: candidate: ’template<class Coll, class T> requires ...’
     14 | void add(Coll& coll, const T& val)
         |      ^~~
prog.cpp:14:6: note: template argument deduction/substitution failed:
prog.cpp:14:6: note: constraints not satisfied
prog.cpp: In substitution of ’template<class Coll, class T> requires ...
           [with Coll = std::set<int>; T = int]’:
prog.cpp:27:4: required from here
prog.cpp:8:9: required for the satisfaction of ’SupportsPushBack<Coll, T>’
                  [with Coll = std::set<int, std::less<int>, std::allocator<int> >; T = int]
prog.cpp:8:28: in requirements with ’Coll c’, ’T v’
                      [with T = int; Coll = std::set<int, std::less<int>, std::allocator<int> >]
prog.cpp:9:16: note: the required expression ’c.push_back(v)’ is invalid
      9 | c.push_back(v);
         | ~~~~~~~~~~~^~~
\end{shell}
}

However, when passing atomics, the check for being copyable is still detected in the deep darkness of the code for std::vector<> (this time when the concept is checked, instead of when the code is compiled).

In that case, the situation improves when we specify the basic constraint for the use of push\_back() as a requirement instead:

\begin{cpp}
template<typename Coll, typename T>
requires std::convertible_to<T, typename Coll::value_type>
void add(Coll& coll, const T& val)
{
	coll.push_back(val);
}
\end{cpp}

Here, we use the standard concept std::convertible\_to to require that the type of the passed argument T can be (implicitly or explicitly) converted to the element type of the collection.

Now, if the requirement is broken, we can get an error message with the broken concept and the location where it is broken. For example[This is just one example of a possible message, not necessarily matching the situation of a specific compiler]:

{\footnotesize
\begin{shell}
...
prog.cpp:11:17: In substitution of ’template<class Coll, class T>
                 requires convertible_to<T, typename Coll::value_type>
                 void add(Coll&, const T&)
                [with Coll = std::vector<std::atomic<int> >; T = std::atomic<int>]’:
prog.cpp:25:18: required from here
.../include/concepts:72:13: required for the satisfaction of
                 ’convertible_to<T, typename Coll::value_type>
                  [with T = std::atomic<int>;
                      Coll = std::vector<std::atomic<int>,
                                              std::allocator<std::atomic<int> > >]’
.../include/concepts:72:30: note: the expression ’is_convertible_v<_From, _To>
                      [with _From = std::atomic<int>; _To = std::atomic<int>]’
                      evaluated to ’false’
    72 | concept convertible_to = is_convertible_v<_From, _To>
        |                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
...
\end{shell}
}

See rangessort.cpp for another example of algorithms that check the constraints of their parameters.

\mySubsubsection{3.3.2}{Using Concepts to Disable Generic Code}

Assume we want to provide a special implementation for an add() function template like the one introduced above. When dealing with floating-point values, something different or extra should happen.

A naive approach might be to overload the function template for double:

\begin{cpp}
template<typename Coll, typename T>
void add(Coll& coll, const T& val) // for generic value types
{
	coll.push_back(val);
}

template<typename Coll>
void add(Coll& coll, double val) // for floating-point value types
{
	... // special code for floating-point values
	coll.push_back(val);
}
\end{cpp}

As expected, when we pass a double as a second argument, the second function is called; otherwise, the generic argument is used:

\begin{cpp}
std::vector<int> iVec;
add(iVec, 42); // OK: calls add() for T being int

std::vector<double> dVec;
add(dVec, 0.7); // OK: calls 2nd add() for double
\end{cpp}

When passing a double, both function overloads match. The second overload is preferred because it is a perfect match for the second argument.

However, if we pass a float, we have the following effect:

\begin{cpp}
float f = 0.7;
add(dVec, f); // OOPS: calls 1st add() for T being float
\end{cpp}

The reason lies in the sometimes subtle details of overload resolution. Again, both functions could be called. Overload resolution has some general rules, such as:

\begin{itemize}
\item
Calls with no type conversion are preferred over calls having a type conversion.

\item
Calls of ordinary functions are preferred over calls of function templates.
\end{itemize}

Here, however, overload resolution has to decide between a call with a type conversion and a call of a function template. By rule, in that case, the version with the template parameter is preferred.

\noindent
\hspace*{\fill} \\ %插入空行
\textbf{Fixing Overload Resolution}

The fix for the wrong overload resolution is pretty simple. Instead of declaring the second parameter with a specific type, we should just require that the value to be inserted has a floating-point type. For this, we can constrain the function template for floating-point values by using the new standard concept std::floating\_point:

\begin{cpp}
template<typename Coll, typename T>
requires std::floating_point<T>
void add(Coll& coll, const T& val)
{
	... // special code for floating-point values
	coll.push_back(val);
}
\end{cpp}

Because we use a concept that applies to a single template parameter we can also use the shorthand notation:

\begin{cpp}
template<typename Coll, std::floating_point T>
void add(Coll& coll, const T& val)
{
	... // special code for floating-point values
	coll.push_back(val);
}
\end{cpp}

Alternatively, we can use auto parameters:

\begin{cpp}
void add(auto& coll, const std::floating_point auto& val)
{
	... // special code for floating-point values
	coll.push_back(val);
}
\end{cpp}

For add(), we now have two function templates that can be called: one without and one with a specific requirement:

\begin{cpp}
uirement:
template<typename Coll, typename T>
void add(Coll& coll, const T& val) // for generic value types
{
	coll.push_back(val);
}

template<typename Coll, std::floating_point T>
void add(Coll& coll, const T& val) // for floating-point value types
{
	... // special code for floating-point values
	coll.push_back(val);
}
\end{cpp}

This is enough because overload resolution also prefers overloads or specializations that have constraints over those that have fewer or no constraints:

\begin{cpp}
std::vector<int> iVec;
add(iVec, 42); // OK: calls add() for generic value types

std::vector<double> dVec;
add(dVec, 0.7); // OK: calls add() for floating-point types
\end{cpp}

\noindent
\hspace*{\fill} \\ %插入空行
\textbf{Do Not Differ More Than Necessary}

If two overloads or specializations have constraints, it is important that overload resolution can decide which one is better. To support this, the signatures should not differ more than necessary.

If the signatures differ too much, the more constrained overload might not be preferred. For example, if we declare the overload for floating-point values to take the argument by value, passing a floating-point value becomes ambiguous:

\begin{cpp}
template<typename Coll, typename T>
void add(Coll& coll, const T& val) // note: pass by const reference
{
	coll.push_back(val);
}

template<typename Coll, std::floating_point T>
void add(Coll& coll, T val) // note: pass by value
{
	... // special code for floating-point values
	coll.push_back(val);
}

std::vector<double> dVec;
add(dVec, 0.7); // ERROR: both templates match and no preference
\end{cpp}

The latter declaration is no longer a special case of the former declaration. We just have two different function templates that could both be called.

If you really want to have different signatures, you have to constrain the first function template not to be available for floating-point values.

\noindent
\hspace*{\fill} \\ %插入空行
\textbf{Restrictions on Narrowing}

We have another interesting issue in this example: both function templates allow us to pass a double to add it into a collection of int:

\begin{cpp}
std::vector<int> iVec;

add(iVec, 1.9); // OOPS: add 1
\end{cpp}

The reason is that we have implicit type conversions from double to int (due to being compatible with the programming language C). Such an implicit conversion where we might lose parts of the value is called narrowing. It means that the code above compiles and converts the value 1.9 to 1 before it is inserted.

If you do not want to support narrowing, you have multiple options. One option is to disable type conversions completely by requiring that the passed value type matches the element type of the collection:

\begin{cpp}
requires std::same_as<typename Coll::value_type, T>
\end{cpp}

However, this would also disable useful and safe type conversions.

For that reason, it is better to define a concept that yields whether a type can be converted to another type without narrowing, which is possible in a short tricky requirement[Thanks to Giuseppe D’Angelo and Zhihao Yuan for the trick to check for narrowing conversions introduced in \url{http://wg21.link/p0870}.]:

\begin{cpp}
template<typename From, typename To>
concept ConvertsWithoutNarrowing =
	std::convertible_to<From, To> &&
	requires (From&& x) {
		{ std::type_identity_t<To[]>{std::forward<From>(x)} }
		-> std::same_as<To[1]>;
	};
\end{cpp}

We can then use this concept to formulate a corresponding constraint:

\begin{cpp}
template<typename Coll, typename T>
requires ConvertsWithoutNarrowing<T, typename Coll::value_type>
void add(Coll& coll, const T& val)
{
	...
}
\end{cpp}

\noindent
\hspace*{\fill} \\ %插入空行
\textbf{Subsuming Constraints}

It would probably be enough to define the concept for narrowing conversions above without requiring the concept std::convertible\_to, because the rest checks this implicitly:

\begin{cpp}
template<typename From, typename To>
concept ConvertsWithoutNarrowing = requires (From&& x) {
	{ std::type_identity_t<To[]>{std::forward<From>(x)} } -> std::same_as<To[1]>;
};
\end{cpp}

However, there is an important benefit if the concept ConvertsWithoutNarrowing also checks for the concept std::convertible\_to. In that case, the compiler can detect that ConvertsWithoutNarrowing is more constrained than std::convertible\_to. The terminology is that ConvertsWithoutNarrowing subsumes std::convertible\_to.

That allows a programmer to do the following:

\begin{cpp}
emplate<typename F, typename T>
requires std::convertible_to<F, T>
void foo(F, T)
{
	std::cout << "may be narrowing\n";
}

template<typename F, typename T>
requires ConvertsWithoutNarrowing<F, T>
void foo(F, T)
{
	std::cout << "without narrowing\n";
}
\end{cpp}

Without specifying that ConvertsWithoutNarrowing subsumes std::convertible\_to, the compiler would raise an ambiguity error here when calling foo() with two parameters that convert to each other without narrowing.

In the same way, concepts can subsume other concepts, which means that they count as more specialized for overload resolution. In fact, the C++ standard concepts build a pretty complex subsumption graph.

We will discuss details of subsumption later.

\mySubsubsection{3.3.3}{Using Requirements to Call Different Functions}

Finally, we should make our add() function template more flexible:

\begin{itemize}
\item
We might also want to support collections that provide only insert() instead of push\_back() for inserting new elements.

\item
We might want to support passing a collection (container or range) to insert multiple values.
\end{itemize}

Yes, you can argue that these are different functions and that they should have different names and if that works, it is often better to use different names. However, the C++ standard library is a good example of the benefits you can get if different APIs are harmonized. For example, you can use the same generic code to iterate over all containers, although internally, containers use very different ways to go to the next element and access its value.

\noindent
\hspace*{\fill} \\ %插入空行
\textbf{Using Concepts to Call Different Functions}

Having just introduced concepts, the “obvious” approach might be to introduce a concept to find out whether a certain function call is supported:

\begin{cpp}
template<typename Coll, typename T>
concept SupportsPushBack = requires(Coll c, T v) {
	c.push_back(v);
};
\end{cpp}

Note that we can also define a concept that needs only the collection as a template parameter:

\begin{cpp}
template<typename Coll>
concept SupportsPushBack = requires(Coll coll, Coll::value_type val) {
	coll.push_back(val);
};
\end{cpp}

Note that we do not have to use typename here to use Coll::value\_type. Since C++20, typename is no longer required when it is clear by the context that a qualified member must be a type.

There are various other ways to declare this concept:

\begin{itemize}
\item
You can use std::declval<>() to get a value of the element type:

\begin{cpp}
template<typename Coll>
concept SupportsPushBack = requires(Coll coll) {
	coll.push_back(std::declval<typename Coll::value_type&>());
};
\end{cpp}

Here, you can see that the definition of concepts and requirements does not create code. It is an unevaluated context where we can use std::declval<>() for “assume we have an object of this type” and it does not matter whether we declare coll as a value or as a non-const reference.

Note that the \& is important here. Without \&, we would only require that we can insert an rvalue (such as a temporary object) using move semantics. With \&, we create an lvalue so that we require that push\_back() copies.

\item
You could use std::ranges::range\_value\_t instead of the value\_type member:

You can use std::declval<>() to get a value of the element type:

\begin{cpp}
template<typename Coll>
concept SupportsPushBack = requires(Coll coll) {
	coll.push_back(std::declval<std::ranges::range_value_t<Coll>>());
};
\end{cpp}

In general, using std::ranges::range\_value\_t<> makes code more generic when the element type of a collection is needed (e.g., it also works with raw array). However, because we require the member push\_back() here, also requiring the member value\_type does not hurt.

\end{itemize}

With the concept SupportPushBack for one parameter, we can provide two implementations:

\begin{cpp}
template<typename Coll, typename T>
requires SupportsPushBack<Coll>
void add(Coll& coll, const T& val)
{
	coll.push_back(val);
}

template<typename Coll, typename T>
void add(Coll& coll, const T& val)
{
	coll.insert(val);
}
\end{cpp}

In this case, we not need a named requirement SupportsInsert here because the add() with the additional requirement is more special, which means that overload resolution prefers it. However, there are only a few containers that support calling insert() with only one argument. To avoid problems with other overloads and calls of add(), we should probably better also have a constraint here.

Because we define the requirement as a concept, we can even use it as a type constraint for the template parameter:

\begin{cpp}
template<SupportsPushBack Coll, typename T>
void add(Coll& coll, const T& val)
{
	coll.push_back(val);
}
\end{cpp}

As a concept, we can also use it as a type constraint for auto as parameter types:

\begin{cpp}
void add(SupportsPushBack auto& coll, const auto& val)
{
	coll.push_back(val);
}

template<typename Coll, typename T>
void add(auto& coll, const auto& val)
{
	coll.insert(val);
}
\end{cpp}

\noindent
\hspace*{\fill} \\ %插入空行
\textbf{Concepts for if constexpr}

We can also use the concept SupportsPushBack directly in an if constexpr condition:

\begin{cpp}
if constexpr (SupportsPushBack<decltype(coll)>) {
	coll.push_back(val);
}
else {
	coll.insert(val);
}
\end{cpp}

\noindent
\hspace*{\fill} \\ %插入空行
\textbf{Combining requires with if constexpr}

We can even skip introducing a concept and pass the requires expression directly as a condition to the compile-time if:

\begin{cpp}
if constexpr (requires { coll.push_back(val); }) {
	coll.push_back(val);
}
else {
	coll.insert(val);
}
\end{cpp}

This is a nice way to switch between two different function calls in generic code. It is especially recommended when introducing a concept is not worthwhile.

\noindent
\hspace*{\fill} \\ %插入空行
\textbf{Concepts versus Variable Templates}

You might wonder why using concepts is better than using a variable template of type bool (like type traits do) such as the following:

\begin{cpp}
template<typename T>
constexpr bool SupportsPushBack = requires(T coll) {
	coll.push_back(std::declval<typename T::value_type>());
};
\end{cpp}

Concepts have the following benefits:

\begin{itemize}
\item
They subsume.

\item
They can be used as type constraints directly in front of template parameters or auto.

\item
They can be used with a compile-time if when using ad-hoc requirements.
\end{itemize}

If you do not need any of these benefits, the question of whether to prefer the definition of a concept or a variable template of type bool becomes interesting and this is discussed later in detail.

\noindent
\hspace*{\fill} \\ %插入空行
\textbf{Inserting Single and Multiple Values}

To provide an overload that deals with multiple values passed as one collection, we can simply add constraints for them. The standard concept std::ranges::input\_range can be used for that:

\begin{cpp}
template<SupportsPushBack Coll, std::ranges::input_range T>
void add(Coll& coll, const T& val)
{
	coll.insert(coll.end(), val.begin(), val.end());
}

template<typename Coll, std::ranges::input_range T>
void add(Coll& coll, const T& val)
{
	coll.insert(val.begin(), val.end());
}
\end{cpp}

Again, as long as the overload has this as an additional constraint, these functions will be preferred.

The concept std::ranges::input\_range is a concept introduced to deal with ranges, which are collections you can iterate over with begin() and end(). However, ranges are not required to have begin() and end() as member functions. Code that deals with ranges should therefore use the helpers std::ranges::begin() and std::ranges::end() that the ranges library provides:

\begin{cpp}
template<SupportsPushBack Coll, std::ranges::input_range T>
void add(Coll& coll, const T& val)
{
	coll.insert(coll.end(), std::ranges::begin(val), std::ranges::end(val));
}
template<typename Coll, std::ranges::input_range T>
void add(Coll& coll, const T& val)
{
	coll.insert(std::ranges::begin(val), std::ranges::end(val));
}
\end{cpp}

These helpers are function objects so that using them avoids ADL problems.

\noindent
\hspace*{\fill} \\ %插入空行
\textbf{Dealing with Multiple Constraints}

By bringing together all useful concepts and requirements, we could place them all in one function at different locations.

\begin{cpp}
template<SupportsPushBack Coll, std::ranges::input_range T>
requires ConvertsWithoutNarrowing<std::ranges::range_value_t<T>,
typename Coll::value_type>
void add(Coll& coll, const T& val)
{
	coll.insert(coll.end(),
				std::ranges::begin(val), std::ranges::end(val));
}
\end{cpp}

To disable narrowing conversions, we use std::ranges::range\_value\_t to pass the element type of the ranges to ConvertsWithoutNarrowing. std::ranges::range\_value\_t is another ranges utility for getting the element type of ranges when iterating over them.

We could also formulate them together in the requires clause:

\begin{cpp}
template<typename Coll, typename T>
requires SupportsPushBack<Coll> &&
			std::ranges::input_range<T> &&
			ConvertsWithoutNarrowing<std::ranges::range_value_t<T>,
							typename Coll::value_type>
void add(Coll& coll, const T& val)
{
	coll.insert(coll.end(),
				std::ranges::begin(val), std::ranges::end(val));
}
\end{cpp}

Both ways of declaring the function template are equivalent.

\mySubsubsection{3.3.4}{The Example as a Whole}

The previous subsections provided a huge amount of flexibility. So, let us bring all options together to have at least one full example as a whole:

\filename{lang/add.cpp}

\begin{cpp}
#include <iostream>
#include <vector>
#include <set>
#include <ranges>
#include <atomic>

// concept for container with push_back():
template<typename Coll>
concept SupportsPushBack = requires(Coll coll, Coll::value_type val) {
	coll.push_back(val);
};

// concept to disable narrowing conversions:
template<typename From, typename To>
concept ConvertsWithoutNarrowing =
	std::convertible_to<From, To> &&
	requires (From&& x) {
		{ std::type_identity_t<To[]>{std::forward<From>(x)} }
		-> std::same_as<To[1]>;
	};


// add() for single value:
template<typename Coll, typename T>
requires ConvertsWithoutNarrowing<T, typename Coll::value_type>
void add(Coll& coll, const T& val)
{
	if constexpr (SupportsPushBack<Coll>) {
		coll.push_back(val);
	}
	else {
		coll.insert(val);
	}
}
// add() for multiple values:
template<typename Coll, std::ranges::input_range T>
requires ConvertsWithoutNarrowing<std::ranges::range_value_t<T>,
									typename Coll::value_type>
void add(Coll& coll, const T& val)
{
	if constexpr (SupportsPushBack<Coll>) {
		coll.insert(coll.end(),
					std::ranges::begin(val), std::ranges::end(val));
	}
	else {
		coll.insert(std::ranges::begin(val), std::ranges::end(val));
	}
}

int main()
{
	std::vector<int> iVec;
	add(iVec, 42); // OK: calls push_back() for T being int
	
	std::set<int> iSet;
	add(iSet, 42); // OK: calls insert() for T being int
	
	short s = 42;
	add(iVec, s); // OK: calls push_back() for T being short
	
	long long ll = 42;
	// add(iVec, ll); // ERROR: narrowing
	// add(iVec, 7.7); // ERROR: narrowing
	
	std::vector<double> dVec;
	add(dVec, 0.7); // OK: calls push_back() for floating-point types
	add(dVec, 0.7f); // OK: calls push_back() for floating-point types
	// add(dVec, 7); // ERROR: narrowing
	
	// insert collections:
	add(iVec, iSet); // OK: insert set elements into a vector
	add(iSet, iVec); // OK: insert vector elements into a set
	
	// can even insert raw array:
	int vals[] = {0, 8, 18};
	add(iVec, vals); // OK
	// add(dVec, vals); // ERROR: narrowing
}
\end{cpp}

As you can see, I decided to use the concept SupportsPushBack for one template parameter inside the various function templates with if constexpr.

\mySubsubsection{3.3.5}{Former Workarounds}

Constraining templates was already possible before C++20. However, the approaches for doing this were often not easy to use and could provide significant drawbacks.

\noindent
\hspace*{\fill} \\ %插入空行
\textbf{SFINAE}

The major approach for disabling the availability of templates before C++20 was SFINAE. The term “SFINAE” (pronounced like sfee-nay) stands for “substitution failure is not an error” and means the rule that we simply ignore generic code if its declaration is not well-formed instead of raising a compile-time error.

For example, to switch between push\_back() and insert(), we could declare the function templates before C++20 as follows:

\begin{cpp}
template<typename Coll, typename T>
auto add(Coll& coll, const T& val) -> decltype(coll.push_back(val))
{
	return coll.push_back(val);
}

template<typename Coll, typename T>
auto add(Coll& coll, const T& val) -> decltype(coll.insert(val))
{
	return coll.insert(val);
}
\end{cpp}

Because we make the call to push\_back() here part of the first template declaration, this template is ignored if push\_back() is not supported. The corresponding declaration is necessary in the second template for insert() (overload resolution does ignore return types and would complain if both function templates can be used).

However, that is a very subtle way to place a requirement and programmers might overlook it easily.

\noindent
\hspace*{\fill} \\ %插入空行
\textbf{std::enable\_if<>}

For more complicated cases of disabling generic code, since C++11, the C++ standard library provides std::enable\_if<>.

This is a type trait that takes a Boolean condition that yields invalid code if false. By using the std::enable\_if<> somewhere inside a declaration, this could also “SFINAE out” generic code.

For example, you could use the std::enable\_if<> type trait to exclude types from calling the add() function template in the following way:

\begin{cpp}
// disable the template for floating-point values:
template<typename Coll, typename T,
typename = std::enable_if_t<!std::is_floating_point_v<T>>>
void add(Coll& coll, const T& val)
{
	coll.push_back(val);
}
\end{cpp}

The trick is to insert an additional template parameter to be able to use the trait std::enable\_if<>. The trait yields void if it does not disable the template (you can specify that it yields another type as an optional second template parameter).

Code like this is also hard to write and read and has subtle drawbacks. For example, to provide another overload of add() with a different constraint, you would need yet another template parameter. Otherwise, you would have two different overloads of the same function because for the compiler std::enable\_if<> does not change the signature. In addition, you have to take care that for each call exactly one of the different overloads is available.

Concepts provide a far more readable way of formulating constraints. This includes that overloads of a template with different constraints do not violate the one definition rule as long as only one constraint is met and that even multiple constraints can be met provided one constraint subsumes another constraint.




